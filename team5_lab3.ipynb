{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import socket\n",
    "import pandas as pd\n",
    "import numpy as ny\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import os \n",
    "import time\n",
    "\n",
    "\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score,brier_score_loss, precision_score, recall_score,f1_score)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "is_rohit=socket.gethostname()=='Rohits-MacBook-Pro.local'\n",
    "is_blake=socket.gethostname()=='BJH-ML-machine'\n",
    "is_neddy=(is_rohit+is_blake==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(is_neddy):\n",
    "    df_business_eda = pd.read_pickle(\"~/Documents/yelp_datasets/df_business_eda_proto4.pickle\")\n",
    "\n",
    "if(is_rohit):\n",
    "    df_business_eda = pd.read_pickle(\"~/Documents/yelp_datasets/df_business_eda_proto4.pickle\")\n",
    "\n",
    "if(is_blake):\n",
    "    df_business_eda = pd.read_pickle(\"df_business_eda.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETL\n",
    "categorical_cols=df_business_eda.select_dtypes(include=['object']).columns\n",
    "\n",
    "df_business_hot=df_business_eda\n",
    "\n",
    "for col in categorical_cols:\n",
    "    dummies=pd.get_dummies(df_business_hot[col], dummy_na=True, prefix=col)\n",
    "    df_business_hot=df_business_hot.\\\n",
    "        drop(col,axis=1).\\\n",
    "    merge(\n",
    "        dummies,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "df_business_hot=df_business_hot.fillna(False)\n",
    "\n",
    "df_business_hot[\"above_average\"] = ny.where(df_business_hot[\"stars\"] > 3.5, 0, 1)\n",
    "y=df_business_hot.rating_category\n",
    "y2 = df_business_hot[\"above_average\"]\n",
    "X=df_business_hot.drop([\"rating_category\", \"stars\",\"above_average\"], axis=1)\n",
    "#Scale your data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X) \n",
    "X_scaled = pd.DataFrame(scaler.transform(X),columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation \n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ms.StratifiedShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         random_state=123,\n",
    "                         test_size  = 0.2)\n",
    "\n",
    "def cv_train(name,model,x,y):\n",
    "    print(f\"====Performing Cross Validation for {name}\")\n",
    "    print(f\" Iteration \", end = '')\n",
    "    iter_num=0\n",
    "    res=pd.DataFrame()\n",
    "\n",
    "    for train_indices, test_indices in cv_object.split(x,y):\n",
    "        t = time.time()\n",
    "        print(f\" {iter_num},\", end = '')\n",
    "\n",
    "        X_train = x.iloc[train_indices]\n",
    "        y_train = y.iloc[train_indices]\n",
    "\n",
    "        X_test = x.iloc[test_indices]\n",
    "        y_test = y.iloc[test_indices]\n",
    "\n",
    "        model.fit(X_train,y_train)  # train object\n",
    "        y_hat = model.predict(X_test) # get test set precitions\n",
    "\n",
    "        conf = mt.confusion_matrix(y_test,y_hat)\n",
    "        \n",
    "        row=pd.DataFrame([iter_num],columns=['Iteration'])\n",
    "        row[\"accuracy\"]=accuracy_score(y_test, y_hat)\n",
    "        row[\"precision\"]=precision_score(y_test, y_hat, average=\"macro\")\n",
    "        row[\"recall\"]=recall_score(y_test, y_hat, average=\"macro\")\n",
    "        row[\"f1\"]=f1_score(y_test, y_hat, average=\"macro\")\n",
    "        row[\"average_seconds\"]= ny.round(time.time() - t)\n",
    "\n",
    "        res=res.append(row)\n",
    "        iter_num+=1\n",
    "    \n",
    "    #Summarize CV Results \n",
    "    summary=res.drop(\"Iteration\",axis=1).agg(\"mean\").to_frame().T\n",
    "    #summary.insert(0,\"model\",model)\n",
    "    summary.insert(0,\"name\",name)\n",
    "    print(\"  Cross-validation complete\")\n",
    "\n",
    "    return(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out Balanced classweight option\n",
    "log_newton_model = LogisticRegression(penalty='l2', C=1.0, class_weight=\"balanced\", solver='newton-cg' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Performing Cross Validation for log_newton\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n"
     ]
    }
   ],
   "source": [
    "cv_results = cv_train(\"log_newton\",log_newton_model, X_scaled, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca_nb = Pipeline(\n",
    "    [('PCA',PCA(n_components=30,svd_solver='randomized')),\n",
    "     ('CLF',GaussianNB())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pca_knn1 = Pipeline(\n",
    "    [('PCA',PCA(n_components=100,svd_solver='randomized')),\n",
    "     ('CLF',KNeighborsClassifier(n_neighbors=1))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5 = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_random_models = {\"knn5\":knn5,\"pca_nb\":pca_nb,\"pca_knn1\":pca_knn1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Performing Cross Validation for knn5\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n",
      "====Performing Cross Validation for pca_nb\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n",
      "====Performing Cross Validation for pca_knn1\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n"
     ]
    }
   ],
   "source": [
    "for name,model in other_random_models.items():\n",
    "    cv_results=cv_results.append(cv_train(name,model,X_scaled,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_estimators = 50\n",
    "\n",
    "tree_based_models = {\n",
    "    'Stump':             DecisionTreeClassifier(max_depth=1, min_samples_leaf=1),\n",
    "    'Tree':              DecisionTreeClassifier(),\n",
    "    'Random Trees':      RandomForestClassifier(max_depth=50, n_estimators=num_estimators),\n",
    "    'Extra Random Trees': ExtraTreesClassifier(n_estimators=num_estimators,min_samples_split=2),\n",
    "    'Boosted Tree':       GradientBoostingClassifier(n_estimators=num_estimators), #takes a long time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Performing Cross Validation for Stump\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n",
      "====Performing Cross Validation for Tree\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n",
      "====Performing Cross Validation for Random Trees\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n",
      "====Performing Cross Validation for Extra Random Trees\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n",
      "====Performing Cross Validation for Boosted Tree\n",
      " Iteration  0, 1, 2,  Cross-validation complete\n"
     ]
    }
   ],
   "source": [
    "for name,model in tree_based_models.items():\n",
    "    cv_results=cv_results.append(cv_train(name,model,X_scaled,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Performing Cross Validation for mlp_adam\n",
      " Iteration  0, 1, 2,"
     ]
    }
   ],
   "source": [
    "#Neural Networks \n",
    "from sklearn.neural_network import MLPClassifier #no parallel processing option so slow \n",
    "\n",
    "nn_based_models = {\n",
    "        'mlp_adam': MLPClassifier(hidden_layer_sizes=(8,8,8), activation='relu', solver='adam', max_iter=500)\n",
    "}\n",
    "for name,model in nn_based_models.items():\n",
    "    cv_results=cv_results.append(cv_train(name,model,X_scaled,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>average_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>perceptron_adam</td>\n",
       "      <td>0.708346</td>\n",
       "      <td>0.708234</td>\n",
       "      <td>0.708213</td>\n",
       "      <td>0.708185</td>\n",
       "      <td>76.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Trees</td>\n",
       "      <td>0.696066</td>\n",
       "      <td>0.695980</td>\n",
       "      <td>0.695637</td>\n",
       "      <td>0.695688</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boosted Tree</td>\n",
       "      <td>0.690643</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.690089</td>\n",
       "      <td>0.690147</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>log_newton</td>\n",
       "      <td>0.686672</td>\n",
       "      <td>0.686698</td>\n",
       "      <td>0.686023</td>\n",
       "      <td>0.686067</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extra Random Trees</td>\n",
       "      <td>0.676678</td>\n",
       "      <td>0.676536</td>\n",
       "      <td>0.676202</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>knn5</td>\n",
       "      <td>0.634581</td>\n",
       "      <td>0.634410</td>\n",
       "      <td>0.634424</td>\n",
       "      <td>0.634405</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tree</td>\n",
       "      <td>0.631044</td>\n",
       "      <td>0.630877</td>\n",
       "      <td>0.630902</td>\n",
       "      <td>0.630880</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stump</td>\n",
       "      <td>0.597424</td>\n",
       "      <td>0.607836</td>\n",
       "      <td>0.593180</td>\n",
       "      <td>0.581279</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pca_knn1</td>\n",
       "      <td>0.580205</td>\n",
       "      <td>0.580052</td>\n",
       "      <td>0.580069</td>\n",
       "      <td>0.580036</td>\n",
       "      <td>13.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pca_nb</td>\n",
       "      <td>0.491324</td>\n",
       "      <td>0.576812</td>\n",
       "      <td>0.502869</td>\n",
       "      <td>0.339932</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  accuracy  precision    recall        f1  \\\n",
       "0     perceptron_adam  0.708346   0.708234  0.708213  0.708185   \n",
       "0        Random Trees  0.696066   0.695980  0.695637  0.695688   \n",
       "0        Boosted Tree  0.690643   0.690600  0.690089  0.690147   \n",
       "0          log_newton  0.686672   0.686698  0.686023  0.686067   \n",
       "0  Extra Random Trees  0.676678   0.676536  0.676202  0.676250   \n",
       "0                knn5  0.634581   0.634410  0.634424  0.634405   \n",
       "0                Tree  0.631044   0.630877  0.630902  0.630880   \n",
       "0               Stump  0.597424   0.607836  0.593180  0.581279   \n",
       "0            pca_knn1  0.580205   0.580052  0.580069  0.580036   \n",
       "0              pca_nb  0.491324   0.576812  0.502869  0.339932   \n",
       "\n",
       "   average_seconds  \n",
       "0        76.333333  \n",
       "0         8.000000  \n",
       "0        11.000000  \n",
       "0         6.000000  \n",
       "0        11.000000  \n",
       "0        16.666667  \n",
       "0         1.000000  \n",
       "0         0.000000  \n",
       "0        13.666667  \n",
       "0         1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.sort_values('accuracy',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are to build upon the predictive analysis (classification) that you already completed in the previous mini-project, adding additional modeling from new classification algorithms as well as more explanations that are inline with the CRISP-DM framework. You should use appropriate cross validation for all of your analysis (explain your chosen method of performance validation in detail). Try to use as much testing data as possible in a realistic manner (you should define what you think is realistic and why)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You must identify two tasks from the dataset to regress or classify. That is:\n",
    "• Two classification tasks OR\n",
    "• Two regression tasks OR\n",
    "• One classification task and one regression task\n",
    "For example, if your dataset was from the diabetes data you might try to predict two tasks: (1) classifying if a patient will be readmitted within a 30 day period or not, and (2) regressing what the total number of days a patient will spend in the hospital, given their history and specifics of the encounter like tests administered and previous admittance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation (15 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [10 points] Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [5 points] Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation (70 points total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [10 points] Choose and explain your evaluation metrics that you will use (i.e., accuracy,\n",
    "precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [10 points] Choose the method you will use for dividing your data into training and\n",
    "testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why\n",
    "your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [20 points] Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [10 points] Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [10 points] Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> • [10 points] Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
